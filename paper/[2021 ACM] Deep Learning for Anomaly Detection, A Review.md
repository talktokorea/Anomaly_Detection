# Deep Learning for Anomaly Detection: A Review

read date: 2022ë…„ 3ì›” 25ì¼
Author: GUANSONG PANG and CHUNHUA SHEN
Journal: ACM
PDF: Deep%20Learn%2020e25/2021_ACM_Deep_Learning_for_Anomaly_Detection_A_Review.pdf
Published Date: March 2021
detail: Review ë…¼ë¬¸
- Major Problem Complexities
- Main Challenges Tackled
- Categorization of Deep Anomaly Detection
  1. Deep Learning for Feature Extraction
  2. Learning Feature Representations of Normality
  3. End-to-end Anomaly Score Learning
keyword: Anomaly Detection, survey
link: https://dl.acm.org/doi/pdf/10.1145/3439950
status: Finished!

# Abstract

- Anomaly Detection ì¡°ì‚¬ë¥¼ ì•„ë˜ì™€ ê°™ì´ í•˜ì˜€ë‹¤.
    1. í¬ê´„ì  ë¶„ë¥˜
    2. ë°œì „ : 3 high-level ì¹´í…Œê³ ë¦¬
    3. ë°©ë²•ë¡  : 11ê°œ  ì¹´í…Œê³ ë¦¬ë¡œ ë‚˜ëˆˆ
    
- ë…¼ì˜ ë‚´ìš©
    1. key intuitions : í•µì‹¬ ì§ê´€
    2. objective functions : ëª©ì  í•¨ìˆ˜
    3. underlying assumptions : ê¸°ë³¸ì  ì „ì œ ì¡°ê±´
    4. advantages and disadvantages : ì¥ì ê³¼ ë‹¨ì 
    5. a set of possible future opportunities : ê°€ëŠ¥í•œ ë¯¸ë˜ ê¸°íšŒë“¤ì˜ ëª¨ìŒ
    6. new perspectives on addressing the challenges
    

# INTRODUCTION

- ì´ìƒì¹˜ íƒì§€ëŠ” ë‹¤ìˆ˜ì˜ ì¸ìŠ¤í„´ìŠ¤ ë°ì´í„°ë¡œë¶€í„° ë§¤ìš° ë²—ì–´ë‚œ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê²€ì¶œí•˜ëŠ” í”„ë¡œì„¸ìŠ¤ë¼ê³  ë¶ˆë¦¬ë©°, 1960ë…„ëŒ€ ì´ˆê¸° ì—°êµ¬ë¡œë¶€í„° ìˆ˜ì‹­ ë…„ ë™ì•ˆ í™œë°œí•˜ê²Œ(active) ì—°êµ¬ë˜ì—ˆë‹¤.
- ì ìš© ë„ë©”ì¸ : ë„“ì€ ë¶„ì•¼ì—ì„œ ì ìš© í•„ìš”ì„±ì´ ì¦ê°€ ë˜ê³  ìˆë‹¤.

<aside>
ğŸ’¡ risk management, compliance(ê·œì • ì¤€ìˆ˜), security, financial surveillance(ì¬ì • ê°ì‹œ), health and medical risk, and AI safety

</aside>

- ë‹¤ì–‘í•œ ì»¤ë®¤ë‹ˆí‹° : ì ì  ë” ì¤‘ìš”í•œ ì—­í• ì„ í•˜ê³  ìˆë‹¤.

<aside>
ğŸ’¡ data mining, machine learning, computer vision, and statistics

</aside>

- ë”¥ëŸ¬ë‹ì€ ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„°ì— ëŒ€í•´ ëŒ€ë‹¨í•œ í‘œí˜„ í•™ìŠµì„ ë³´ì¸ë‹¤.

<aside>
ğŸ’¡ high-dimensional, temporal(ì‹œê°„), spatial(ê³µê°„), graph                                                         â†’ pushing the boundaries of different learning tasks(ë‹¤ì–‘í•œ í•™ìŠµë“¤ì˜ ê²½ê³„ë¥¼ í—ˆë¬¼ë‹¤)

</aside>

- deep ì´ìƒì¹˜íƒì§€ aim(ëª©í‘œ) : NNë¥¼ í†µí•´ feature representations or anomaly scores í•™ìŠµ
    - ë§ì€ deep anomaly detectionì´ ì‹¤ë¬´ì—ì„œ ê¸°ì¡´ì˜ ì´ìƒ íƒì§€ë³´ë‹¤ í›¨ì”¬ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„
    
- 5ê°œì˜ ì£¼ìš” ê¸°ì—¬ì  : Why we need deep anomaly detection?
    1. Problem nature and challenges (ë¬¸ì œì˜ ë³¸ì§ˆê³¼ ê³¼ì œ)
    2. Categorization and formulation (ë¶„ë¥˜ ë° ê³µì‹í™”) 
        1. deep learning for generic(ì¼ë°˜) feature extraction
        2. learning representations of normality(ì •ê·œì„±)
        3. end-to-end anomaly score learning : end-to-end ì´ìƒ ì ìˆ˜ í•™ìŠµ
    3. Comprehensive literature review(í¬ê´„ì  ë¬¸í—Œ ë¦¬ë·°)
    4. Future opportunities : discuss about implication(ì˜í–¥)
    5. Source codes and datasets : real anomalies data-set

# ANOMALY DETECTION
: PROBLEM COMPLEXITIES AND CHALLENGES

- ëª…ë°±í•œ íŒ¨í„´ taskì™€ëŠ” ë‹¬ë¦¬ ì´ìƒì¹˜ ê°ì§€ëŠ” ì†Œìˆ˜, ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥/ë¶ˆí™•ì‹¤ì„± ë° í¬ê·€ ì´ë²¤íŠ¸ë¥¼ í•´ê²°í•˜ê¸°ì— ëª¨ë“ (ì‹¬ì¸µ ë˜ëŠ” ì–•ì€) ê°ì§€ ë°©ë²•ì— ê³ ìœ í•œ ë¬¸ì œê°€ ë³µì¡í•´ì§„ë‹¤.

1. **Major Problem Complexities (ì£¼ìš” ë¬¸ì œ)**
    1. Unknownness (ì•Œë ¤ì§€ì§€ ì•ŠìŒ)
    2. Heterogeneous anomaly classes (ì—¬ëŸ¬ ì¢…ë¥˜ì˜ í´ë˜ìŠ¤ë“¤)
    3. Rarity and class imbalance (í¬ê·€ì„±ê³¼ ë¶ˆê· í˜•)
    4. Diverse types of anomaly (ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì´ìƒì¹˜)
        1. Point anomalies : ê°œë³„ ì¸ìŠ¤í„´ìŠ¤
        2. Conditional anomalies : in a specific context (ex. ê¸‰ê²©í•œ ì˜¨ë„ ê°•í•˜/ìƒìŠ¹)
        3. Group anomalies : ëŒ€ê·œëª¨ë¡œ ë°œìƒ í•¨ (ex. ê°€ì§œ ê³„ì •ë“¤, ë””ë„ìŠ¤ ê³µê²©)
    
2. **Main Challenges Tackled by Deep Anomaly Detection (ì£¼ìš” ê³¼ì œ)**
    1. CH1: Low anomaly detection recall rate. : ë‚®ì€ ì´ìƒì¹˜ ê°ì§€ìœ¨ (ì´ìƒì¹˜ ìˆ˜ ìì²´ê°€ ì ê¸° ë•Œë¬¸)
        - false positives â†“, detection recall rates â†‘
        - significant expense of failing to spotting anomalies.
    2. CH2: Anomaly detection in high-dimensional and/or not-independent data : ê³ ì°¨ì›, ë¹„ë…ë¦½
        - ì € ì°¨ì›ì—ì„œ ëª…ë°±í•œ ì´ìƒì¹˜ì˜ íŠ¹ì„±ì„ ë³¼ ìˆ˜ ìˆì§€ë§Œ, ê³ ì°¨ì›ì—ì„œëŠ” ëˆˆì— ë›°ì§€ ì•ŠëŠ”ë‹¤.
        - solution : subspace-based, feature selection-based methods
        - preserved proper information : ë¯¸ì§€/ì´ì§ˆì„± ë•Œë¬¸ì— ë³´ì¥ì´ ì–´ë ¤ì›€
        - temporal, spatial, graph-based : ì¢…ì†ì ì¸ ì¸ìŠ¤í„´ìŠ¤ëŠ” ì´ìƒ ê°ì§€ê°€ ì–´ë ¤ì›€
    3. CH3: Data-efficient learning of normality/abnormality : ë°ì´í„°-íš¨ìœ¨ì ì¸ í•™ìŠµ
        - fully supervised anomaly detectionì€ ë¹„í˜„ì‹¤ì ì„ ë¹„ìš© å¤š
        - Semi-supervised anomaly detection : normal dataë§Œ ê°–ê³  ìˆìŒ
        - unsupervised methods do not have any prior knowledge(ì‚¬ì „ ì§€ì‹) of true anomalies.
            
             ã„´ assumption on the distribution of anomaliesì— í¬ê²Œ ì˜ì¡´í•˜ê³  ìˆë‹¤. ë…¸ì´ì¦ˆì— ë¯¼ê°
            
        - weakly supervised anomaly detection : ì •ë°€í•˜ì§€ ì•Šê±°ë‚˜ ë¶€ì •í™•í•œ í´ë˜ìŠ¤ ë¼ë²¨ì´ ìˆìŒ
    4. CH4: Noise-resilient anomaly detection : ë…¸ì´ì¦ˆì— ê°•í•¨
        - ë ˆì´ë¸”ì´ ì˜ëª»ëœ ë°ì´í„° ë˜ëŠ” ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ ì´ìƒì¹˜
    5. CH5: Detection of complex anomalies : ë³µì¡í•œ ì´ìƒì¹˜ ê²€ì¶œ
        - ê¸°ì¡´ ë°©ë²•ë¡ ì˜ ëŒ€ë‹¤ìˆ˜ì¸ Point anomaliesë¥¼ Conditional & Groupì— ì ìš©í•˜ê¸° ì–´ë ¤ì›€
    6. CH6: Anomaly explanation : ì´ìƒì¹˜ ì„¤ëª…
        - ë¸”ë™ë°•ìŠ¤ ëª¨ë¸ì€ ë¦¬ìŠ¤í¬ê°€ í¬ë‹¤.  ë“œë¬¸(=ë ˆì–´) ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ í¸í–¥ì„ ë°°ìš¸ ìˆ˜ ìˆìŒ
        - main challenge  : well balance the modelâ€™s interpretability and effectiveness : ê²€ì¶œ ëŠ¥ë ¥ì´ ë‹¤ê°€ ì•„ë‹˜
            
            â†’ Ranking Model ìœ ì¼í•˜ê²Œ í•´ê²°í•˜ë©°, ê²°ë¡  5.ë¥¼ ë³´ë©´ ì´ ë¬¸ì œë¥¼ ì¶”ëŠ” ì „ìš© DL ëª¨ë¸ í•„ìš”ì„±ì— ëŒ€í•´ ëŒ€ë‘í•˜ê³  ìˆìŒ
            
3. PROBLEM COMPLEXITIES AND CHALLENGES ì •ë¦¬

![Untitled](Deep%20Learn%2020e25/Untitled.png)

- Deep methods ì“°ë©´ ê¸°ì¡´ê³¼ ë‹¬ë¦¬ end-to-end ìµœì í™” &  íŠ¹í™”ëœ í‘œí˜„ í•™ìŠµì´ ê°€ëŠ¥
- Intricate Relation Learning : ë³µì¡í•œ ê´€ê³„ í•™ìŠµ
- Heterogeneity Handling : ì—¬ëŸ¬ ë‹¤ë¥¸ ì¢…ë¥˜ ë°ì´í„° ë‹¤ë£¨ëŠ” ê²ƒ

# ADDRESSING THE CHALLENGES WITH DEEP ANOMALY DETECTION

1. Preliminaries (ì˜ˆë¹„)
    - Activation functions
        - linear, sigmoid, tanh, Rectified Linear Unit (ReLU)
    - layer
        - fully connected(MLP), convolutional + pooling(CNN), recurrent(RNN) layers
    - dataset
        
        ![Untitled](Deep%20Learn%2020e25/Untitled%201.png)
        
    - representation space
        
        ![Untitled](Deep%20Learn%2020e25/Untitled%202.png)
        
    - feature representation mapping function : deep anomaly detectionì´ í•™ìŠµ ëª©í‘œë¡œ í•˜ëŠ” ê²ƒ
        
        ![Untitled](Deep%20Learn%2020e25/Untitled%203.png)
        
        - í•™ìŠµì„ ìœ„í•´ ê° ì¸ìŠ¤í„´ìŠ¤ì˜ ì´ìƒì¹˜ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ ì¶”ê°€ ë‹¨ê³„ í•„ìš”
    - anomaly score learning function
        
        ![Untitled](Deep%20Learn%2020e25/Untitled%204.png)
        
        - raw data(ì›ì‹œ ë°ì´í„°) ì…ë ¥ìœ¼ë¡œ ì§ì ‘ ì´ìƒì¹˜ ì ìˆ˜ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŒ
        - í° $\tau$ëŠ” ë” í° ì´ìƒ ì •ë„ë¥¼ ë‚˜íƒ€ëƒ„
    - $\phi$ ì™€ $\tau$ëŠ” hidden layer ì™€ ì•„ë˜ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ì´ ìˆëŠ” ì‹ ê²½ë§ ì§€ì› mapping functionì´ë‹¤.
        
        ![Untitled](Deep%20Learn%2020e25/Untitled%205.png)
        

1. Categorization of Deep Anomaly Detection (ì¹´í…Œê³ ë¦¬ í™”)
- 3ê°œì˜ ë©”ì¸ ì¹´í…Œê³ ë¦¬ì™€ 11ê°œì˜ ì„¸ë°€í•œ ì¹´í…Œê³ ë¦¬ë¡œ ëª¨ë¸ì˜ ê´€ì ì—ì„œ ë¶„ë¥˜í•¨

![Untitled](Deep%20Learn%2020e25/Untitled%206.png)

![Untitled](Deep%20Learn%2020e25/Untitled%207.png)

1) Deep Learning for Feature Extraction : hybridì˜ ê²½ìš° ì´ ë¶€ë¶„ì„ ì„  ì²˜ë¦¬ í›„ ML ì§„í–‰ ë¨

: independent(ë…ë¦½ì ) feature extractors only

2) Learning Feature Representations of Normality : ì•„ë˜ 2ê°œ ë°©ë²•ì´ ì„œë¡œ ì˜ì¡´ì 

- ì•„ë˜ 2ê°œì˜ ë°©ë²•ì€ ê±°ë¦¬ ë° í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ ì¸¡ì •(measure)ì„ í•˜ëŠ” ì§€ì— ë”°ë¼ ë‚˜ë‰œë‹¤.
1. Generic normality feature learning : ì¼ë°˜ì ì¸ ì •ìƒ í•ì³ í•™ìŠµ
2. Anomaly measure-dependent feature learning :  : ë¹„ì •ìƒ ì¢…ì†-ì¸¡ì • í•ì³ í•™ìŠµ

3) End-to-end Anomaly Score Learning 

: end-to-end ë°©ì‹ìœ¼ë¡œ ì´ìƒì¹˜ ì ìˆ˜ë¥¼ í•™ìŠµí•˜ëŠ”ë° ì „ë…í•¨

# DEEP LEARNING FOR FEATURE EXTRACTION

![Untitled](Deep%20Learn%2020e25/Untitled%208.png)

- extract low-dimensional feature representations from high-dimensional and/or non-linearly separable data for downstream anomaly detection
    
    : ìµœì¢…(downstram) ì´ìƒì¹˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë¶„ë¦¬í•  ìˆ˜ ìˆëŠ” ë¹„ì„ í˜• ë°ì´í„° ë˜ëŠ” ê³ ì°¨ì›ìœ¼ë¡œë¶€í„° feature representation)ì„ ì €ì°¨ì›ìœ¼ë¡œ ë½‘ì•„ë‚¸ë‹¤.
    
- feature extraion ê³¼ anomaly scroingì€ ë…ë¦½ì ìœ¼ë¡œ ë¶„ë¦¬ë¨
    
    â†’ ë”¥ëŸ¬ë‹ êµ¬ì„± ìš”ì†ŒëŠ” ìˆœì „íˆ ì°¨ì›ì¶•ì†Œë¡œ ì‘ë™í•¨. anomaly scroingì€ ìƒˆë¡œìš´ ê³µê°„ì— ì ìš© ì‚°ì¶œ
    

![Untitled](Deep%20Learn%2020e25/Untitled%209.png)

- Assumptions.
    - The feature representations preserve discriminative information : ì‹ë³„ ì •ë³´ ë³´ì¡´
- ì°¨ì› ì¶•ì†Œ ë°©ë²•(PCA, random projection)ë“¤ì— ë¹„í•´ ë”¥ëŸ¬ë‹ ê¸°ë²•ì´ ë” ë‚˜ì€ ëŠ¥ë ¥ì„ ê°–ê³  ìˆë‹¤.
- ì¢…ë¥˜ : AlexNet / VGG / ResNet / Unmasking framework / a linear one-class SVM
- Advantages
    1.  A large number of state-of-theart (pre-trained) deep models and off-the-shelf anomaly detectors are readily available. : ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ DL modelì´ ë§ë‹¤
    2. Deep feature extraction offers more powerful dimensionality reduction than popular linear methods. : ìœ ëª…í•œ ì„ í˜• ë°©ë²•ë³´ë‹¤ Deep feature extrationì´ ì°¨ì› ì¶•ì†Œì— ë” ê°•í•˜ë‹¤.
    3. It is easy-to-implement given the public availability of the deep models and detection methods. : ê°ì§€ ë°©ë²•ê³¼ ë”¥ ëª¨ë¸ì˜ ê³µê°œë¥¼ ê³ ë ¤í–ˆì„ ë•Œ êµ¬í˜„ì´ ìš©ì´í•˜ë‹¤.
- Disadvantages.
    1. The fully disjointed feature extraction and anomaly scoring often lead to suboptimal anomaly scores. : í•ì³ ì¶”ì¶œê³¼ ì´ìƒì¹˜ ì ìˆ˜ê°€ ì™„ì „íˆ ë¶„ë¦¬ë˜ì–´ ìµœì ì´ ì•„ë‹Œ ì ìˆ˜ê°€ ë‚˜ì˜¤ëŠ” ê²½ìš°ê°€ ë§ë‹¤.
    2. Pre-trained deep models are typically limited to specific types of data : ì‚¬ì „ í›ˆë ¨ëœ ë”¥ ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ íŠ¹ì • íƒ€ì…ì˜ ë°ì´í„°ë¡œ ì œí•œëœë‹¤.
- Challenges Targeted(ëŒ€ìƒ ê³¼ì œ)  :
    1. CH2(ê³ ì°¨ì›/ë¹„ë…ë¦½ì„±) : ê³ ì°¨ì›/ë¹„ë…ë¦½ ë°ì´í„°ë¥¼ ë‚®ì€ ì°¨ì› ê³µê°„ì— íˆ¬ì˜í•˜ì—¬ ë‹¨ìˆœí•œ ë°ì´í„° ê³µê°„ì—ì„œ AD ê°€ëŠ¥. 
    2. CH1(ë‚®ì€ ì´ìƒì¹˜ ê°ì§€ìœ¨) : ì—¬ëŸ¬ ìœ í˜•ì˜ í•ì³ë¥¼ í™œìš©í•˜ê³  í‘œí˜„ì´ í’ë¶€í•œ ê²€ì¶œ ëª¨ë¸ì„ í•™ìŠµí•´ false positiveë¥¼ ì¤„ì´ëŠ”ë° ë„ì›€ì´ ë¨

# LEARNING FEATURE REPRESENTATIONS OF NORMALITY

![Untitled](Deep%20Learn%2020e25/Untitled%2010.png)

1. Generic Normality Feature Learning
    
    : ì¼ë°˜ ê¸°ëŠ¥ í•™ìŠµ ëª©ì  í•¨ìˆ˜ë¥¼ ìµœì í™”í•˜ì—¬ ë°ì´í„° ì¸ìŠ¤í„´ìŠ¤ì˜ í‘œí˜„ì„ í•™ìŠµ
    
      AE / GAN / Predictability modeling / SSL classfication
    

![Untitled](Deep%20Learn%2020e25/Untitled%2011.png)

- ë°ì´í„° ì¬êµ¬ì„±, ìƒì„± ëª¨ë¸ë§, ì˜ˆì¸¡ ê°€ëŠ¥ì„± ëª¨ë¸ë§, ìì²´ ì§€ë„ ë¶„ë¥˜ ë°©ë²• ë“± ì—¬ëŸ¬ ê´€ì ì´ í¬í•¨ë¨
    - $\phi$ : í‘œí˜„ ê³µê°„ Zì— ì›ë³¸ë°ì´í„° xë¥¼ ë§¤í•‘í•¨
    - $\psi$ : Wì— ì˜í•´ íŒŒë¼ë¯¸í„°í™” ë˜ì—ˆìœ¼ë©°, Zê³µê°„ì—ì„œ ë™ì‘í•˜ê³ , ë°ì´í„° ê·œì¹™ì„± í•™ìŠµì„ ì‹œí–‰í•˜ëŠ”ë° ì „ë…í•˜ëŠ” ëŒ€ìš© í•™ìŠµ ì‘ì—…ì´ë‹¤.
    - $\ell$ : ê¸°ë³¸ ëª¨ë¸ë§ ì ‘ê·¼ ë°©ì‹ ê´€ë ¨ ì†ì‹¤ í•¨ìˆ˜
    - $\ f$ : $\phi$ ì™€ $\psi$ë¥¼ í™•ìš©í•˜ì—¬ $s_x$ë¥¼ ê³„ì‚°í•˜ëŠ” scoring funtion

- Autoencoders (AE)
    
    : ì£¼ì–´ì§„ ë°ì´í„°ê°€ ì˜ ì¬êµ¬ì„± ë  ìˆ˜ ìˆëŠ” ì¼ë¶€ ì € ì°¨ì› ê¸°ëŠ¥ í‘œí˜„ ê³µê°„ì„ ë°°ì›€. reconstrucion errorë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ë°ì´í„°ì˜ ì¤‘ìš” ê·œì¹™ì„±ì„ í•™ìŠµí•˜ë„ë¡ ê°•ì œ ë¨. ì´ìƒì¹˜ëŠ” ì´ëŸ° ì—ëŸ¬ê°€ í¼
    
    - Assumption
        
        : ì •ìƒ ì¸ìŠ¤í„´ìŠ¤ëŠ” ì´ìƒì¹˜ë³´ë‹¤ ê³µê°„ì—ì„œ ë” ì˜ ì¬êµ¬ì„± ë˜ê³  ì••ì¶•ëœë‹¤.
        
    - network : encoding(ê¸°ì¡´ ë°ì´í„° â†’ ì €ì°¨ì› ê³µê°„) + decoding (ì €ì°¨ì› â†’ ë˜ëŒë¦¬ê¸° ì‹œë„)
    - ì´ìƒë°ì´í„°ì˜ ê²½ìš° reconstructed ì˜ ì•ˆë¨(reconstrucion errorë¥¼ anomlay scoreë¡œ ì‚¬ìš© ê°€ëŠ¥)
        
        ![Untitled](Deep%20Learn%2020e25/Untitled%2012.png)
        
        - $\phi_e$ : $\theta_e$ ë¥¼ ê°€ì§„ ì¸ì½”ë”© ë„¤íŠ¸ì›Œí¬, $\phi_d$ : $\theta_d$ë¥¼ ê°€ì§„ ë””ì½”ë”© ë„¤íŠ¸ì›Œí¬
        
        ![Untitled](Deep%20Learn%2020e25/Untitled%2013.png)
        
        - ì¸ì½”ë”©ê³¼ ë””ì½”ë”ëŠ” ë™ì¼í•œ ê°€ì¤‘ì¹˜ íŒŒë¼ë¯¸í„°ë¥¼ ê³µìœ í•˜ì—¬ íŒŒë¼ë¯¸í„° ê°ì†Œì™€ ì •ê·œí™”í•  ìˆ˜ ìˆë‹¤.
        
        ![Untitled](Deep%20Learn%2020e25/Untitled%2014.png)
        
        - $s_x$ : xì˜ reconstruction error ê¸°ë°˜ì˜ ì´ìƒì¹˜ ì ìˆ˜
    - AE ì¢…ë¥˜ : sparse/Denoising/Contractive/Variational/RandNet/RDA
    - ë³µì¡í•œ ë°ì´í„°ì—ì„œ AE ì‚¬ìš© ì‹œ
        
        : CNN-AE, LSTM-AE, Conv-LSTM-AE, graph convolutional network-AE
        
    - Advantages.
        1. The idea of AEs is straightforward and generic to different types of data. 
            
            : AEì˜ ê°œë…ì€ ë‹¤ì–‘í•œ íƒ€ì…ì˜ ë°ì´í„°ì— ëŒ€í•´ ê°„ë‹¨í•˜ê³  ì¼ë°˜ì ì„
            
        2. Different types of powerful AE variants can be leveraged to perform anomaly detection. : ë‹¤ì–‘í•œ ìœ í˜•ì˜ AE ë³€í˜• ëª¨ë¸ë“¤ì´ ì¡´ì¬
    - Disadvantages.
        1. The learned feature representations can be biased : í¸í–¥ë  ìˆ˜ ìˆìŒ
        2. ëª©ì í•¨ìˆ˜ê°€ ê¸°ë³¸ ê·œì¹™ì„±ì˜ ì¼ë°˜ì  ìš”ì•½ì´ê³  ì´ìƒ ê²€ì¶œ ëª©ì ì´ ì•„ë‹ˆë¼ì„œ ë¶ˆê·œì¹™ì„± ê²€ì¶œì— ìµœì í™”ê°€ ì•„ë‹˜
    - Challenges Targeted (Challenge : í•´ê²° ë‚´ìš©)
        1. CH2(ê³ ì°¨ì›/ë¹„ë…ë¦½ì„±) : ë‹¤ì–‘í•œ AE ì•„í‚¤í…ì³ ì‚¬ìš©ìœ¼ë¡œ graph & multivariate sequence dataì— ì ìš© ê°€ëŠ¥
        2. CH1(ë‚®ì€ ì´ìƒì¹˜ ê°ì§€ìœ¨) : ê¸°ì¡´ ë°©ë²•ë¡  ë³´ë‹¤ í‘œí˜„ë ¥ì´ ë†’ê¸° ë•Œë¬¸
        3. CH4(ë…¸ì´ì¦ˆì— ê°•í•¨) : AEëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì•½í•¨, RPCA+AEë¡œ ì‚¬ìš©í•˜ë©´ ê°•í•´ì§ˆ ìˆ˜ ìˆìŒ 
        
- Generative Adversarial Networks (GAN)
    - DADë¡œ ìœ ëª…í•¨. latent spaceì˜ normality ì˜ í¬ì°©í•˜ëŠ” ê²ƒ ëª©í‘œ.
    - Anomaly score = ì‹¤ì œ ì¸ìŠ¤í„´ìŠ¤ì™€ ìƒì„±ëœ ì¸ìŠ¤í„´ìŠ¤ì˜ residual
    - ì¢…ë¥˜ : AnoGAN / EBGAN / BiGAN / ALAD / GANomaly / Wasserstein GAN / Cycle GAN
    - GAN ëª©ì  í•¨ìˆ˜ : min-max game
        
        ![Untitled](Deep%20Learn%2020e25/Untitled%2015.png)
        
        - ì™¼ìª½ : ì›ë³¸ ë°ì´í„° ì¤‘ì— ì—¬ëŸ¬ ê°œë¥¼ ë½‘ê³  ë¡œê·¸ í›„ í‰ê·  ê°’ì„ ì·¨í•˜ê² ë‹¤.
        - ì˜¤ë¥¸ìª½ : ë…¸ì´ì¦ˆ ë²¡í„°ë¥¼ ë½‘ê³  ìƒì„±ìì— ë„£ì–´ì„œ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ê³  ê·¸ê±¸ íŒë³„í•œ ë’¤ í™•ë¥  ê°’ì„ ë¡œê·¸ í›„ í‰ê·  ê°’
        - D(discriminator), G(generator), V(value function), ê° xì— ëŒ€í•´ ìµœê³ ì˜ zë¥¼ ì°¾ìŒ
        - xì˜ GAN loss ê¸°ë°˜ì˜ ì´ìƒì¹˜ ì ìˆ˜
            
            ![Untitled](Deep%20Learn%2020e25/Untitled%2016.png)
            
            - $\alpha$ : hyperparameter, $\gamma^{*}$ : ë§ˆì§€ë§‰ ë‹¨ê³„
        - ì†ì‹¤ í•¨ìˆ˜ 1) residual loss
            
            ![Untitled](Deep%20Learn%2020e25/Untitled%2017.png)
            
        - ì†ì‹¤ í•¨ìˆ˜ 2) discrimination loss
            
            ![Untitled](Deep%20Learn%2020e25/Untitled%2018.png)
            
            - $\gamma$ = index of search iteration, $\ h$ = feature mapping ìˆ˜í–‰ (Dì˜ ì¤‘ê°„ ë ˆì´ì–´ì—ì„œ)
    - Advantages.
        1. ì‚¬ì‹¤ì  ì¸ìŠ¤í„´ìŠ¤ì„ ìƒì„±í•˜ëŠ” íƒì›”í•œ ëŠ¥ë ¥ìœ¼ë¡œ ì ì¬ ê³µê°„ì—ì„œ ì¬êµ¬ì„± ë˜ì§€ ì•ŠëŠ” ë¹„ì •ìƒì ì¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ íƒì§€í•  ìˆ˜ ìˆìŒ
        2. ë§ì€ ì¢…ë¥˜ì˜ GAN ë°©ë²•ë“¤ì´ ì´ìƒì¹˜ íƒì§€ì— ì ìš©ë  ìˆ˜ ìˆìŒ
    - Disadvantages.
        1. ìˆ˜ë ´ ì‹¤íŒ¨ì™€ ëª¨ë“œ ë¶•ê´´ì™€ ê°™ì€ ë¬¸ì œë¡œ ëª¨ë¸ í›ˆë ¨ì— í° ì–´ë ¤ì›€ì„ ê²ªì„ ìˆ˜ ìˆìŒ
        2.  real data ë¶„í¬ê°€ ë³µì¡í•˜ê±°ë‚˜ íŠ¹ì´ì¹˜ê°€ í¬í•¨ëœ ê²½ìš°, ìƒì„± ë„¤íŠ¸ì›Œí¬ëŠ” ì˜ëª»ëœ ê¸¸ë¡œ ì´ëŒ ìˆ˜ ìˆë‹¤.
        3. data synthesis(í•©ì„±)ì„ ìœ„í•´ ì„¤ê³„ë˜ì—ˆê¸° ë•Œë¬¸ì— ADì—ëŠ” ìµœì„ ì´ ì•„ë‹ ìˆ˜ ìˆë‹¤.
    - Challenges Targeted   (Challenge : í•´ê²° ë‚´ìš©)
        1. CH1(ë‚®ì€ ì´ìƒì¹˜ ê°ì§€ìœ¨) : ì ì¬ ê³µê°„ì´ ì£¼ìš” ì´ìƒì¹˜ íŒë‹¨ ì •ë³´ë¥¼ ë³´ì¡´í•  ë•Œ, ê¸°ì¡´ ë°ì´í„° ê³µê°„ì„ ë„˜ì–´ ì •í™•ë„ ê°ì§€ê°€ í–¥ìƒë¨
        2. CH2(ê³ ì°¨ì›/ë¹„ë…ë¦½ì„±) : í•™ìŠµëœ ì €ì°¨ì› ì ì¬ ê³µê°„ì˜ reconstructionì„ ì¡°ì‚¬í•˜ì—¬ ê³ ì°¨ì›ì˜ ì´ìƒì¹˜ë“¤ì„ ê²€ì¶œí•  ìˆ˜ ìˆìŒ
        
- Predictability Modeling (ì˜ˆì¸¡ê°€ëŠ¥ ëª¨ë¸ë§)
    
    : ì´ì „ ì¸ìŠ¤í„´ìŠ¤ í‘œí˜„ì„ ë¬¸ë§¥ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ í˜„ì¬ë¥¼ ì˜ˆì¸¡ (ex. video frames in a video sequence. ë¹„ë””ì˜¤ ì´ìƒ ê²€ì¶œì—ì„œ good)
    
    - ì •ìƒì€ ì¢…ì†ì„±ì— ì˜ ì¢…ì†ë˜ì–´ ìˆì–´ ì˜ˆì¸¡ ê°€ëŠ¥, ë¹„ì •ìƒì€ ì¢…ì¢… ì¢…ì†ì„±ì„ ìœ„ë°˜í•˜ì—¬ ì˜ˆì¸¡ ë¶ˆê°€
    - Assumption.
        
        : ì •ìƒì¹˜ëŠ” ì¼ì‹œì ìœ¼ë¡œ ì˜ˆì¸¡ì´ ìš©ì´
        
    - ë¹„ë””ì˜¤ í”„ë ˆì„ì˜ ì¶©ë¶„í•œ ì˜ˆì¸¡ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ prediction objective functionì—ì„œ ëª¨ì–‘ê³¼ ë™ì‘ íŠ¹ì§•ì— ëŒ€í•œ ë‹¤ë¥¸ ì œì•½ì´ í•„ìš”
    - ì¢…ë¥˜ : U-Net / autoregressive model
    - general objective function for video fram
        
        ![Untitled](Deep%20Learn%2020e25/Untitled%2019.png)
        
        - $\ell_{pred}$ : frame prediction loss by mse,   $\ell_{adv}$ : adversarial loss
        - $\hat{x}_{t+1}$ : predict future frame,   ${x}_{t+1}$ : ground truth
            
            ![Untitled](Deep%20Learn%2020e25/Untitled%2020.png)
            
            - $\psi$ : function for the frame generation
    - Peak Signal-to-Noise Ratioë¥¼ ì‚¬ìš©í•´ $\left\| x_i = \hat{x}_{i} \right\|_2$ ë¡œ Anomaly scoreë¥¼ ì •ì˜í•¨
    - í”„ë ˆì„ ì˜ˆì¸¡ì„ ë” ë†’ì´ê¸° ìœ„í•´ AE ê¸°ë°˜ì˜ reconstruction networkê°€ ì¶”ê°€ë˜ê¸°ë„ í•œë‹¤.
    - Advantages.
        1. ë‹¤ìˆ˜ì˜ ì‹œí€€ìŠ¤ í•™ìŠµ ê¸°ë²•ì´ ì´ ì ‘ê·¼ ë°©ì‹ì— ì ìš© ë  ìˆ˜ ìˆìŒ
        2. ì´ ì ‘ê·¼ ë°©ì‹ì€ ë‹¤ì–‘í•œ ì‹œê°„ì , ê³µê°„ì  ì˜ì¡´ì„± í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•¨
    - Disadvantages.
        1. ì´ ë°©ì‹ì€ ì—°ì† ë°ì´í„°ì—ì„œì˜ ì´ìƒì¹˜ íƒì§€ì— í•œì •ëœë‹¤.
        2. ì—°ì† ì˜ˆì¸¡ ê³„ì‚° ì†Œìš” å¤š
        3. ìˆœì°¨ì  ì˜ˆì¸¡ì„ ê¸°ì´ˆí•˜ê¸° ë•Œë¬¸ì— ì´ìƒ ê²€ì¶œì— ì°¨ì„ ì¼ ìˆ˜ ìˆìŒ
    - Challenges Targeted
        1. CH1 and CH2 : ê³ ì°¨ì› ë° ì‹œê°„ ë°ì´í„°ì˜ ì´ìƒ íƒì§€ì˜ false positiveë¥¼ í•´ê²°í•˜ëŠ”ë° ë„ì›€
        2. CH5(ë³µì¡í•œ ì´ìƒì¹˜ ê²€ì¶œ) : ì‹œê°„ì  ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¡°ê±´ë¶€ ì´ìƒ íƒì§€ ê°€ëŠ¥
        
- Self-supervised Classification
    
    : ë¶„ë¥˜ ëª¨ë¸ì„ ìƒì„±í•˜ë©´ì„œ ì •ìƒì˜ í‘œí˜„ì„ ë°°ìš°ê³ , ë¶„ë¥˜ ëª¨ë¸ì´ ì¼ì¹˜ ë˜ì§€ ì•ŠëŠ” ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì´ìƒì¹˜ë¡œ êµ¬ë¶„í•œë‹¤.
    
    - ì „í†µì  ê¸°ë²•ì¸ Cross-Feature Analysis, feature modelsì— ê¸°ë°˜í•¨
    - ì „í†µì ì¸ ëª¨ë¸ì€ ì›ë³¸ì˜ tabular dataë¥¼ ì‚¬ìš©í•˜ëŠ” ë°˜ë©´, deep consistency-based anomaly detectionì€ augmented ëœ image dataì— ì´ˆì ì„ ë§ì¶°ì„œ ì˜ˆì¸¡ ëª¨ë¸ì„ êµ¬ì¶•í•¨
    - augmented instanceë¥¼ íš¨ê³¼ì ìœ¼ë¡œ íŒë³„í•˜ê¸° ìœ„í•´ í›ˆë ¨ ë°ì´í„°ì˜ íŒ¨í„´ ë¬˜ì‚¬ê°€ ì¤‘ìš”
    - Assumptions.
        - ì •ìƒ ì¸ìŠ¤í„´ìŠ¤ëŠ” ìê¸°ì§€ë„ ë¶„ë¥˜ê¸°ì—ì„œ ì´ìƒì¹˜ ë³´ë‹¤ ë” ê°•í•˜ê²Œ ì¼ì¹˜ ë¨
    - normality score loss function
        
        ![Untitled](Deep%20Learn%2020e25/Untitled%2021.png)
        
        - CE = standard cross-entropy loss function.
        - $\psi$ = multi-class classifier parameterized with W
        - ${y_{T}}_{j}$ = ë³€í™˜ ìœ í˜• T_jì— ì˜í•´ argumented ëœ synthetic classì˜ one-hot encoding
        - z = instace xê°€ T(ë³€í™˜ ìœ í˜•)ì— ë”°ë¼ì„œ argumentedëœ ì €ì°¨ì› feature representation
            
            ![Untitled](Deep%20Learn%2020e25/Untitled%2022.png)
            
            - $\phi$ = í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ í†µí•œ feature learner
    - classification score í•©ì‚°í•˜ì—¬ anomaly score ê³„ì‚°(ì¼ê´€ì„± ìœ„í•´ ê°€ì • : Dirichlet distribution)
        - negative entropy-based anomaly scores perform better than average, maximum
    - Advantages.
        1. They work well in both the unsupervised and semi-supervised settings. 
            
            : ë¹„ì§€ë„, ì¤€ì§€ë„ í•™ìŠµì—ì„œ ì˜ ì‘ë™ë¨
            
        2. Anomaly scoring is grounded by some intrinsic properties of gradient magnitude and its updating : ì´ìƒì¹˜ ì ìˆ˜ëŠ” ê¸°ìš¸ê¸° í¬ê¸°ì™€ ì—…ë°ì´íŠ¸ì— ì˜í•´ ê¸°ë°˜ ëœë‹¤.
    - Disadvantages
        1. above transformationì€ ì´ë¯¸ì§€ ë°ì´í„°ì—ë§Œ ì ìš©ë˜ë©°, ì¢…ì¢… ë°ì´í„°ì— ì˜ì¡´ì ì´ë‹¤.
        2. classification modelì€ end-to-end ë°©ì‹ì´ê¸° ë•Œë¬¸ì—, an integrated module in the optimization(ìµœì í™” í†µí•© ëª¨ë“ˆ)ì´ ì•„ë‹ˆë¼ ë¶„ë¥˜ ì ìˆ˜ì— ë”°ë¼ ë„ì¶œë¨ìœ¼ë¡œ ì´ìƒì¹˜ ì ìˆ˜ê°€ ìµœì í™”ê°€ ë˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.
    - Challenges Targeted
        1. CH1 and CH2 : ì •ê·œì„± í‘œí˜„ì  ì €ì°¨ì› í‘œí˜„ì´ ê¸°ì¡´ ê³ ì°¨ì› ê³µê°„ë³´ë‹¤ ì´ìƒ ê°ì§€ì— ë” ë„ìŒ
        2. CH4(ë…¸ì´ì¦ˆ) : ì •ìƒê³¼ ì´ìƒ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ì´ì˜ ë³¸ì§ˆì  ì°¨ì´ë¡œ unsupervisedì—ì„œ ì‘ë™ ê°€ëŠ¥í•˜ë©°, ë…¸ì´ì¦ˆì— ëŒ€í•´ good robustness
        
1. Anomaly Measure-dependent Feature Learning
    
    : ê¸°ì¡´ì˜ anomaly measure í•˜ë‚˜ì— íŠ¹ë³„íˆ ìµœì í™”ëœ feature representationì„ í•™ìŠµí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•¨
    
      Distance-based Measure / One-class Classification-based Measure / Clustering-based Measure
    

- Distance-based Measure
    - Distance-based methods are straightforward and easy-to-implement.
    - ì¢…ë¥˜ : DB outliers / k-nearest neighbor distance / average k-nearest neighbor distance / relative distance
        
        â†’ ì°¨ì›ì˜ ì €ì£¼ ë•Œë¬¸ì˜ ì „í†µì  ë°©ë²•ë“¤ì€ ê³ ì°¨ì› ë°ì´í„°ì—ì„œ ì˜ ì‘ë™ë˜ì§€ ì•Šì•˜ìŒ
        
    - deep distance-based anomaly detectionì€ ì €ì°¨ì› ê³µê°„ íˆ¬ì˜ìœ¼ë¡œ í•œê³„ë¥¼ ì˜ ê·¹ë³µí•  ìˆ˜ ìˆìŒ
        - ì¢…ë¥˜ : random nearest neighbor distance
    - Assumption.
        - ì´ìƒì¹˜ëŠ” ì •ìƒì¹˜ë¡œë¶€í„° ë°€ë„ ì£¼ë³€ì—ì„œ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆë‹¤.
    - hinge loss function
        
        ![Untitled](Deep%20Learn%2020e25/Untitled%2023.png)
        
        - S = subset of x
        - A & N = Anomlay & Normal instance Set
        - m =  $\ f(x, S; \theta)$ì—ì„œ ì‚°ì¶œëœ ë‘ ê±°ë¦¬ ì‚¬ì´ì˜ ì—¬ë°± ê´€ë ¨ ì •ì˜ ëœ constant(ìƒìˆ˜)
            
            ![Untitled](Deep%20Learn%2020e25/Untitled%2024.png)
            
            - $\phi$ = projected representation space
    - Advantages.
        1. ê±°ë¦¬ê¸°ë°˜ ì´ìƒ íƒì§€ëŠ” ê°„ë‹¨í•˜ê³  ì´ë¡ ì´ ì˜ ì •ì˜ë˜ì–´ ìˆìŒ. ê·¸ëŸ¬ë¯€ë¡œ deep distance-based anomaly detection methodsì€ ì¶©ë¶„í•œ ê·¼ê±°ê°€ ë  ìˆ˜ ìˆë‹¤.
        2. ì €ì°¨ì› í‘œí˜„ ê³µê°„ì—ì„œ ì‘ë™í•˜ê³  ê¸°ì¡´ì— ì‹¤íŒ¨í•œ ê³ ì°¨ì› ë°ì´í„°ì— íš¨ê³¼ì ì¼ ìˆ˜ ìˆìŒ
        3. ì´ìƒì¹˜ì— íŠ¹ë³„í•˜ê²Œ ë§ì¶°ì§„ í‘œí˜„ì„ ë°°ìš¸ ìˆ˜ ìˆë‹¤.
    - Disadvantages.
        1. ê´‘ë²”ìœ„í•œ ì—°ì‚°ì€ í‘œí˜„ í•™ìŠµ ê³¼ì •ì—ì„œ ê±°ë¦¬ ê¸°ë°˜ ì´ìƒ ì¸¡ì • í†µí•©ì— ì¥ì• ê°€ ë  ìˆ˜ ìˆìŒ
        2. distance-based anomaly measures ê³ ìœ  ì•½ì ìœ¼ë¡œ ê¸°ëŠ¥(capabilities)ì´ ì œí•œ ë  ìˆ˜ ìˆë‹¤.
    - Challenges Targeted.
        1. CH1 and CH2 : distance-based detectionì˜ ì°¨ì›ì˜ ì €ì£¼ë¥¼ í•´ê²°í•˜ê³  ì € ì°¨ì› í‘œí˜„ì„ í•™ìŠµ í•  ìˆ˜ ìˆìŒ
        2. CH3(íš¨ìœ¨ì  í•™ìŠµ) : ë¼ë²¨ë§ ëœ ëª‡ ê°€ì§€ì˜ ì´ìƒì¹˜ë¥¼ ì´ìš©í•˜ì—¬ íš¨ê³¼ì  ì •ê·œì„± í‘œí˜„ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ê³ ì•ˆë  ìˆ˜ ìˆë‹¤.
        3. CH4(ë…¸ì´ì¦ˆ) : pseudo ì´ìƒì¹˜ ë¼ë²¨ì˜ ì´ì ì€ ì ì¬ì  ì˜¤ì—¼ëœ ì´ìƒì¹˜ì— ë¡œë²„ìŠ¤íŠ¸í•˜ê³  ë¹„ì§€ë„ í•™ìŠµ í™˜ê²½ì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ ì‘ë™í•¨

- One-class Classification-based Measure
    - one-class classification-based ë§ì¶¤í™” ëœ í•ì³ í‘œí˜„ í•™ìŠµ ëª©í‘œ
    - ì¢…ë¥˜ : one-class SVM / SVDD / Conventional one-class SVM / deep SVDD
    - ëª¨ë“  ì •ìƒì¹˜ëŠ” single classì´ë©°, ì´ìƒì¹˜ì— ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ê°„ë‹¨í•œ ëª¨ë¸ë¡œ ìš”ì•½í•  ìˆ˜ ìˆë‹¤.
    - NNì™€ one-class SVMì„ ê²°í•©í•˜ë ¤ëŠ” ìˆ˜ë§ì€ ì‹œë„ê°€ ìˆì—ˆë‹¤.
    - deep one-class SVMì˜ ì•Œë°˜ì  ê³µì‹
        
        ![Untitled](Deep%20Learn%2020e25/Untitled%2025.png)
        
        - r = margin parameter
        - $\theta$ = í‘œí˜„ ë„¤íŠ¸ì›Œí¬ì˜ parameters
        - $\nu$ = í›ˆë ¨ ë°ì´í„°ì—ì„œ ì´ìƒ ë¹„ìœ¨ì˜ ìƒí•œì„ ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°
    - Advantages.
        1. The one-class classification-based anomaliesëŠ” í•™ë¬¸ì ìœ¼ë¡œ ì˜ ì—°êµ¬ë˜ì–´ ì™”ê³ , deep one-class classification-based methodsì˜ ê°•í•œ ê¸°ë°˜ì„ ì œê³µí•œë‹¤.
        2. í‘œí˜„ í•™ìŠµê³¼ one-class classification modelsì„ í†µí•©í•˜ì—¬ ë§ì¶¤í˜•ìœ¼ë¡œ ìµœì ì˜ í‘œí˜„ì„ í•™ìŠµ í•  ìˆ˜ ìˆë‹¤.
        3. one-class modelsì—ì„œ ì ì ˆí•œ ì»¤ë„ í•¨ìˆ˜ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì„ íƒí•˜ì§€ ì•Šì•„ë„ ëœë‹¤.
    - Disadvantages.
        1. The one-class modelsì€ ì •ìƒ í´ë˜ìŠ¤ ì•ˆ ë³µì¡í•œ ë¶„í¬ì˜ ë°ì´í„°ì…‹ì—ëŠ” íš¨ê³¼ì ìœ¼ë¡œ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.
        2. ê°ì§€ ì„±ëŠ¥ì€ one-class classification-based anomaly measuresì— ì˜ì¡´ ëœë‹¤.
    - Challenges Targeted.
        1. CH1 and CH2 : ê°ì§€ ì •í™•ë„ë¥¼ one-class classification modelsì˜ ìµœì í™”ëœ ì €ì°¨ì› í‘œí˜„ ê³µê°„ì„ í•™ìŠµí•˜ì—¬ í–¥ìƒ ì‹œí‚¨ë‹¤.
        2. CH3(íš¨ìœ¨ì  í•™ìŠµ) : ì ê²Œ ë¼ë²¨ë§ëœ ì •ìƒê³¼ ì´ìƒì¹˜ë¥¼ í™œìš©í•˜ì—¬ ë‹¨ì§€ ì•Œë ¤ì§„ ì´ìƒë¿ ì•„ë‹ˆë¼ ìƒˆë¡œìš´ ì´ìƒ í´ë˜ìŠ¤ë¥¼ ê°ì§€ í•  ìˆ˜ ìˆê³  ë” íš¨ê³¼ì ìœ¼ë¡œ one-class ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ ì†Œìˆ˜ì˜ ë ˆì´ë¸” ëœ ì •ìƒ, ë¹„ì •ìƒ ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
        
- Clustering-based Measure
    
    : Deep clustering-based anomaly detectionì€ ìƒˆë¡­ê²Œ í•™ìŠµëœ í‘œí˜„ ê³µê°„ì˜ í´ëŸ¬ìŠ¤í„°ì—ì„œ ë¶„ëª…í•˜ê²Œ ë²—ì–´ë‚œ ì´ìƒì¹˜ì˜ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.
    
    - í´ëŸ¬ìŠ¤í„°ë§ê³¼ ì´ìƒê°ì§€ëŠ” ì„œë¡œ ì—°ê´€ë˜ì–´ ìˆìœ¼ë¯€ë¡œ í´ëŸ¬ìŠ¤í„° í¬ê¸°/ì¤‘ì‹¬ê¹Œì§€ì˜ ê±°ë¦¬/ì¤‘ì‹¬ì‚¬ì´ì˜ ê±°ë¦¬/êµ¬ì„±ì›ê³¼ ê°™ì€ ì´ìƒì¹˜ë¥¼ ì •ì˜í•˜ê¸° ìœ„í•´ ë§ì€ ì—°êµ¬ê°€ ìˆì—ˆë‹¤.
    - ì „í†µì  ë°©ë²• ì¢…ë¥˜ : K-means, GMM, spectral clustering, agglomerative clustering
    - Assumptions.
        - ì •ìƒ ì¸ìŠ¤í„´ìŠ¤ëŠ” ì´ìƒì¹˜ë³´ë‹¤ í´ëŸ¬ìŠ¤í„°ì— ë” ì˜ ì¤€ìˆ˜í•œë‹¤.
        - í´ëŸ¬ìŠ¤í„°ë§ ì„±ëŠ¥ì´ ì…ë ¥ë°ì´í„°ì— í¬ê²Œ ì˜ì¡´í•œë‹¤.
        - ì£¼ìš” 2ê°œì˜ KEY
            1. ì¢‹ì€ í‘œí˜„ì€ ë” ë‚˜ì€ í´ëŸ¬ìŠ¤í„°ë§ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ê³ , í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ê°€ ì¢‹ìœ¼ë©´ í‘œí˜„ í•™ìŠµì— íš¨ê³¼ì ì¸ ê°ì‹œ ì‹ í˜¸ë¥¼ ì œì‹œí•  ìˆ˜ ìˆë‹¤.
            2. í•˜ë‚˜ì˜ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ì„ ìœ„í•´ ìµœì í™”ëœ í‘œí˜„ì€ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ì— ì˜í•´ ê¸°ë³¸ ê°€ì •ì˜ ì°¨ì´ì  ë•Œë¬¸ì— ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ì— ë°˜ë“œì‹œ ìœ ìš©í•˜ì§€ ì•Šë‹¤.
    - The deep clustering methods = forward pass(í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰) + backward pass(í´ëŸ¬ìŠ¤í„° í• ë‹¹)
    - deep clustering methods loss function
        
        ![Untitled](Deep%20Learn%2020e25/Untitled%2026.png)
        
        - $\ell_{clu}$ =  clustering loss function, $\phi$ = feature learner parameterized by $\theta$
        - $\ f$ = clustering assignment function parameterized by $\ W$
        - $\ y_x$ = pseudo class labels yielded(ì‚°ì¶œëœ) by the clustering
        - $\ell_{aux}$ = ì†ì‹¤ í•¨ìˆ˜ì— ì¶”ê°€ì ì¸ ì œí•œì„ ì£¼ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” í´ëŸ¬ìŠ¤í„°ë§ì´ ì•„ë‹Œ ì†ì‹¤ í•¨ìˆ˜
        - $\alpha, \beta$ = ë‘ ì†ì‹¤ì˜ ì¤‘ìš”ë„ë¥¼ í†µì œí•˜ê¸° ìœ„í•œ hyperparameters
    - After the deep clustering, í´ëŸ¬ìŠ¤í„° í• ë‹¹ì„ ì‚¬ìš©í•˜ì—¬ f í•¨ìˆ˜ ê¸°ë°˜ Anomaly score ê³„ì‚° ê°€ëŠ¥
    - í›ˆë ¨ ë°ì´í„°ì…‹ì—ì„œ ì˜¤ì—¼ì´ ìƒê¸°ë©´ í¸í–¥ì´ ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë¹„ì§€ë„ í•™ìŠµì—ì„œ ì ì¬ì  ì´ìƒì¹˜ì˜ ì˜í–¥ ì œê±°ë¥¼ ìœ„í•´ ëª‡ ê°€ì§€ ì¶”ê°€ì  ì œí•œì´ í•„ìš”í•˜ë‹¤.
    - reconstruction error-based handcrafted featuresëŠ” ì´ìƒ íƒì§€ì— deep í´ëŸ¬ìŠ¤í„°ë§ ë³´ë‹¤ ê²°ê³¼í‘œí˜„ì´ ë” ì í•©í•˜ë‹¤.
    - Advantages.
        1. ë§ì€ deep clustering methodsì„ ADì— ì‚¬ìš© ê°€ëŠ¥
        2. ê¸°ì¡´ì˜ í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²•ë“¤ê³¼ ë¹„êµí•˜ì—¬ deep clustering-based methodsì€ ë³µì¡í•œ ë°ì´í„° ì„¸íŠ¸ì˜ ì´ìƒì¹˜ë¥¼ ì‰½ê²Œ ë°œê²¬í•  ìˆ˜ ìˆë„ë¡ íŠ¹ë³„íˆ ìµœì í™”ëœ í‘œí˜„ì„ í•™ìŠµí•¨
    - Disadvantages.
        1. í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ì— ë”°ë¼ ì´ìƒ ê°ì§€ ì„±ëŠ¥ì´ í¬ê²Œ ì˜ì¡´ëœë‹¤.
        2. í´ëŸ¬ìŠ¤í„°ë§ ê³¼ì •ì´ í›ˆë ¨ ë°ì´í„° ì¤‘ ì˜¤ì—¼ëœ ì´ìƒì¹˜ì— ì˜í•´ í¸í–¥ ë˜ì—ˆì„ ìˆ˜ ìˆë‹¤.
    - Challenges Targeted.
        1. CH1 and CH2 : The clustering-based anomaly measureì€ ì…ë ¥ëœ ë°ì´í„°ì˜ ìƒˆë¡­ê²Œ í•™ìŠµëœ ì €ì°¨ì› í‘œí˜„ë“¤ì— ì ìš©ëœë‹¤. íŒë³„ ì •ë³´ê°€ ì¶©ë¶„íˆ í‘œí˜„ ê³µê°„ì— ë³´ì¡´ëœë‹¤ë©´ ê¸°ì¡´ ë°ì´í„° ê³µê°„ë³´ë‹¤ ì¢‹ì€ ê°ì§€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆìŒ
        2. CH4(ë…¸ì´ì¦ˆ) : ì–´ë–¤ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ì€ ì´ìƒì¹˜ë“¤ì— ë¯¼ê°í•˜ê¸°ì— ë”¥ í´ëŸ¬ìŠ¤í„°ë§ê³¼ ìˆœì°¨ì  ì´ìƒ íƒì§€ëŠ” ì£¼ì–´ì§„ ë°ì´í„°ê°€ ì´ìƒì¹˜ì— ì˜í•´ ì˜¤ì—¼ë  ë•Œ, í¬ê²Œ ì˜ëª»ëœ ê¸¸ë¡œ ì´ëŒ ìˆ˜ ìˆë‹¤. AEì˜ ì¬ê±´ì„¤ ì—ëŸ¬ë¡œë¶€í„° ìˆ˜ ì‘ì—… ëœ í•ì³ê°€ ì‚¬ìš©ëœ Deep clusteringì€ ì˜¤ì—¼ì„ í¬í•¨í•œ ë³´ë‹¤ ë¡œë²„ìŠ¤íŠ¸í•œ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ”ë° ë„ì›€ì´ ë  ìˆ˜ ìˆë‹¤.
        

# END-TO-END ANOMALY SCORE LEARNING

![Untitled](Deep%20Learn%2020e25/Untitled%2027.png)

- end-to-end ë°©ì‹ìœ¼ë¡œ ìŠ¤ì¹¼ë¼ AD scoreë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.
- end-to-end anomaly score learning network:
    
    ![Untitled](Deep%20Learn%2020e25/Untitled%2028.png)
    

1. Ranking Models
    - ranking modelì„ ì§ì ‘ í•™ìŠµí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ë©°, ì´ìƒì¹˜ ì ìˆ˜ ì‹ ê²½ë§ì€ ê´€ì¸¡ ê°€ëŠ¥ ìˆœì„œí˜• ë³€ìˆ˜ì— ì˜í•´ ì‹¤í–‰ëœë‹¤.
    - Assumptions.
        - ë°ì´í„° ì´ìƒì„ í¬ì°©í•œ ê´€ì¸¡ ê°€ëŠ¥í•œ ìˆœì„œí˜• ë³€ìˆ˜ê°€ ì¡´ì¬í•œë‹¤.
        - anomaly scoring neural networkì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ ìˆœì„œí˜• regression-based loss functionì´ ê³ ì•ˆë˜ë‹¤. (self-trained deep ordinal regression model for unsupervised video AD)
        - self-trained deep ordinal regression modelì˜ objective function
            
            ![Untitled](Deep%20Learn%2020e25/Untitled%2029.png)
            
        - Advantages.
            1. ì´ìƒì¹˜ ì ìˆ˜ëŠ” ì¡°ì •ëœ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¦‰ì‹œ ìµœì í™” í•  ìˆ˜ ìˆë‹¤.
            2. ì´ë“¤ì€ ë³´í†µ ì´ìƒê³¼ ì •ìƒì˜ ê²½ìš° ì‚¬ì´ì— ìˆœì„œ í˜•ì˜ ì•½í•œ ê°€ì •ì„ í•¨ìœ¼ë¡œ ì´ìƒì¹˜ì— ëŒ€í•œ ì •ì˜ì—ì„œ ììœ ë¡­ë‹¤.
            3. Ranking Models ì ‘ê·¼ ë°©ì‹ì€ ìˆœìœ„ ë§¤ê¸°ê¸° í•™ìŠµê³¼ ê°™ì€ ë¶„ì•¼ì—ì„œ í™•ë¦½ëœ ìˆœìœ„ ë§¤ê¸°ê¸° ê¸°ìˆ ê³¼ ì´ë¡ ì„ ê¸°ë°˜ìœ¼ë¡œ í•  ìˆ˜ ìˆë‹¤.
        - Disadvantages.
            1. ë¼ë²¨ì´ ì§€ì •ëœ ì´ìƒì¹˜ë“¤ì´ í•„ìš”í•˜ë©°, ë¼ë²¨ì´ ì§€ì •ëœ ì´ìƒì¹˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ì‘ìš©(application)ì—ëŠ” ì ìš©ë˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.
            2. ëª¨ë¸ì€ ì†Œìˆ˜ì˜ ë¼ë²¨ë§ ëœ ì´ìƒ ì§•í›„ë¥¼ ê°ì§€í•˜ëŠ” ë°ë§Œ ì í•©ë˜ê¸° ë•Œë¬¸ì— ë¼ë²¨ë§ ëœ ì´ìƒ ì§•í›„ì— ëŒ€í•´ ë‹¤ë¥¸ ë¹„ì •ìƒì ì¸ íŠ¹ì§•ì„ ë‚˜íƒ€ë‚´ëŠ” ë³´ì´ì§€ ì•ŠëŠ” ì´ìƒ ì§•í›„ëŠ” ì¼ë°˜í™”í•˜ì§€ ëª»í•  ìˆ˜ ìˆë‹¤.
        - Challenges Targeted
            1. CH1 and CH2 : ì„ì˜ ë¼ë²¨ì´ë‚˜ ë…¸ì´ì¦ˆ ë¼ë²¨ê³¼ ê°™ì€ weak supervisionì„ ì‚¬ìš©í•˜ë©´. ì˜ì‹¬ ê°€ëŠ” ì´ìƒì¹˜ì˜ ì¤‘ìš”í•œ ì§€ì‹ì„ ì œê³µí•˜ì—¬ ë³´ë‹¤ í‘œí˜„ì ì¸ ì € ì°¨ì› í‘œí˜„ ê³µê°„ê³¼ ë” ë‚˜ì€ íƒì§€ ì •í™•ë„ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•œë‹¤.
            2. CH3(íš¨ìœ¨ì  í•™ìŠµ) : The MIL scheme & pairwise relation predictionì€ ê±°ì¹ ê³ /ì œí•œì ì¸ ì´ìƒì¹˜ ë¼ë²¨ì„ í†µí•©í•  ìˆ˜ ìˆëŠ” ì‰¬ìš´ ë°©ë²•ì„ ì œì‹œí•¨
            3. CH4(ë…¸ì´ì¦ˆ) : ë°ì´í„°ì˜ ë…¸ì´ì¦ˆ ë¼ë²¨ ë˜ëŠ” ì˜¤ì—¼ëœ ì´ìƒì¹˜ê°€  Deep weakly-supervised & Self-trained deep ordinal regression for end-to-end video ë°©ë²•ì—ì„œ ì˜ ì‘ë™í•œë‹¤. 
            4. CH6(ì´ìƒ ì„¤ëª…) : the end-to end anomaly score learningì€ ì´ìƒì¹˜ í™œì„±í™” ê°€ì¤‘ì¹˜ ë˜ëŠ” ì´ìƒ ì ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ì—­ì „íŒŒí•˜ì—¬ í° ì´ìƒ ì ìˆ˜ë¥¼ ë‹´ë‹¹í•˜ëŠ” íŠ¹ì§•ì„ ì°¾ì•„ëƒ„ìœ¼ë¡œì„œ ê°„ë‹¨í•œ ì´ìƒ ì„¤ëª…ì„ ì œê³µí•¨.
        
2. Prior-driven Models
    - ì‚¬ì „ ë¶„í¬(Prior distribution)ë¥¼ ì‚¬ìš©í•˜ì—¬ anomaly score learningì„ ì¸ì½”ë”©í•˜ê³  êµ¬ë™í•¨
    - ì´ìƒ ì ìˆ˜ê°€ end-to-end ë¡œ í•™ìŠµ ë˜ê¸° ë•Œë¬¸ì—, ìš°ì„ ì€ ë‚´ë¶€ ëª¨ë“ˆ ë˜ëŠ” score learning function $\tau$ì˜ ì‚°ì¶œë¬¼ ì¤‘ í•˜ë‚˜ì— ë¶€ê³¼ë  ìˆ˜ ìˆìŒ
    - ì¢…ë¥˜ : Bayesian inverse reinforcement learning (IRL)
    - Assumptions.
        - ì ìš©ëœ  ìš°ì„ ì€ ë°ì´í„°ì…‹ì˜ ê·¼ë³¸ì ì¸ ì´ìƒ/ì •ìƒì„ í¬ì°©í•œë‹¤.
    - IRL optimization function
        
        ![Untitled](Deep%20Learn%2020e25/Untitled%2030.png)
        
        - latent reward function parameterized by $\theta$
            
            ![Untitled](Deep%20Learn%2020e25/Untitled%2031.png)
            
            - $\tau_{\theta}(o, a)$ =  latent reward function parameterized by $\theta$
            - ${\theta},(o, a)$ = a pair of state and action in the sequence s
            - $Z$ = ë§ˆí¬ì½”í”„ ê²°ì • í”„ë¡œì„¸ìŠ¤ ì—­í•™ê³¼ ì¼ì¹˜í•˜ëŠ” ëª¨ë“  ì‹œí€€ìŠ¤ì— ëŒ€í•œ  $\text{exp}(\sum^{}_{(o,a \in s)}t_{\theta} (o,a))$ì˜ ì ë¶„ì˜ ë¶„í•  í•¨ìˆ˜
            - $p(\theta) = \theta$ ì˜ ì‚¬ì „ ë¶„í¬
            - $S$ = ê´€ì°°ëœ ì‹œí€€ìŠ¤ì˜ ì§‘í•©
    - priorì€ contrastive lossë¥¼ ì •ì˜í•˜ëŠ”ë° í™œìš© ë¨
    - Advantages.
        1. ì´ìƒ ì ìˆ˜ëŠ” ì£¼ì–´ì§„ ì‚¬ì „(given prior)ìœ¼ë¡œ ìµœì í™” ë  ìˆ˜ ìˆë‹¤.
        2. ë‹¤ì–‘í•œ ì‚¬ì „ ë¶„í¬ë¥¼ ì´ìƒ ì ìˆ˜ í•™ìŠµì— í†µí•©í•˜ê¸° ìœ„í•œ ìœ ì—°í•œ í”„ë ˆì„ ì›Œí¬ ì œê³µ. ë‹¤ì–‘í•œ Bayesian deep learning techniquesê°€ ì ìš© ë  ìˆ˜ ìˆìŒ
        3. ì‚¬ì „ì€ ë‹¤ë¥¸ ë°©ë²•ë³´ë‹¤ ë” í•´ì„ ê°€ëŠ¥í•œ ì´ìƒ ì ìˆ˜ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìŒ
    - Disadvantages.
        1. ë‹¤ì–‘í•œ ì´ìƒ ê°ì§€ ì ìš© ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•´ ë³´í¸ì ìœ¼ë¡œ íš¨ê³¼ì  ì‚¬ì „ ì„¤ê³„ë¥¼ í•˜ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥ í•˜ì§€ëŠ” ì•Šì§€ë§Œ ì–´ë µìŠµë‹ˆë‹¤.
        2. ëª¨ë¸ì€ ì‚¬ì „ì´ ë¶„í¬ì— ì˜ ë§ì§€ ì•ŠëŠ” ë‹¤ë©´ ëœ íš¨ê³¼ì ìœ¼ë¡œ ì‘ë™í•  ìˆ˜ ìˆë‹¤.
    - Challenges Targeted
        - CH1 and CH2 : ì‚¬ì „ì€  ê³ ì°¨ì› ë° ì‹œí€€ìŠ¤ ê°™ì€ ë‹¤ì–‘í•œ ë³µì¡í•œ ë°ì´í„°ì˜ ì €ì°¨ì› í‘œí˜„ì„ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•œë‹¤.
        - CH1 and CH3(ë‚®ì€ ê°ì§€ìœ¨ íš¨ìœ¨ì  í•™ìŠµ) : ì´ìƒ ì ìˆ˜ì— ëŒ€í•´ ì‚¬ì „ì„ ë¶€ê³¼í•¨ìœ¼ë¡œ, í•œì •ëœ ì–‘ì˜ ë¼ë²¨ëœ ì´ìƒ ë°ì´í„°ë¥¼ í™œìš©í•´ ì •ìƒê³¼ ì´ìƒ í‘œí˜„ì„ í–¥ìƒ ì‹œì¼œ ê°ì§€ìœ¨ì„ í°í­ìœ¼ë¡œ ì˜¬ë¦¬ëŠ” ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚¸ë‹¤.
        - CH4(ë…¸ì´ì¦ˆ) : ê°ì§€ ëª¨ë¸ì€ ì´ìƒ ì ìˆ˜ í•¨ìˆ˜ì˜ ì‚¬ì „ ë¶„í¬ì— ì˜í•´ êµ¬ë™ ë˜ê³ , í›ˆë ¨ ë°ì´í„°ì˜ ì´ìƒì¸ ì˜¤ì—¼ëœ ë°ì´í„°ì—ì„œ ì˜ ì‘ë™í•œë‹¤.
        
3. Softmax Likelihood Models
    - í›ˆë ¨ ë°ì´í„°ì˜ ì‚¬ê±´ ë°œìƒ ê°€ëŠ¥ë„ë¥¼ ê·¹ëŒ€í™”í•˜ì—¬ ì´ìƒ ì ìˆ˜ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì„ ëª©í‘œí•œë‹¤.
    - ì •ìƒì¹˜ëŠ” ë†’ì€ í™•ë¥  ì‚¬ê±´ìœ¼ë¡œ ì¶”ì •ë˜ê³ , ì´ìƒì¹˜ëŠ” ë‚®ì€ í™•ë¥ ì˜ ì‚¬ê±´ì¸ ê²½í–¥ì´ ìˆë‹¤.
        
        â†’ -ì‚¬ê±´ ê°€ëŠ¥ë„(=the negative of the event likelihood)ëŠ” ìì—°íˆ ì´ìƒ ì ìˆ˜ë¡œ ì •ì˜ëœë‹¤.
        
    - ì¢…ë¥˜ : Softmax likelihood(via noise contrastive estimation (NCE))
    - Assumptions.
        - ì´ìƒê³¼ ì •ìƒ ì¸ìŠ¤í„´ìŠ¤ëŠ” ê°ê° ë‚®ì€ í™•ë¥ ê³¼ ë†’ì€ í™•ë¥ ì˜ ì‚¬ì „ì´ë‹¤.
    - ì‚¬ê±´ ê°€ëŠ¥ë„ë¥¼ ëª¨ë¸ë§í•˜ì—¬ ì´ìƒ ì ìˆ˜ë¥¼ í•™ìŠµí•˜ëŠ” í•¨ìˆ˜
        
        ![Untitled](Deep%20Learn%2020e25/Untitled%2032.png)
        
        - $p(x;\theta)$ = í•™ìŠµë  íŒŒë¼ë¯¸í„° $\theta$ì¸ ì¸ìŠ¤í„´ìŠ¤ xì˜ í™•ë¥  = is modeled with a softmax function
            
            ![Untitled](Deep%20Learn%2020e25/Untitled%2033.png)
            
        - $\tau(x;\theta)$ = í•ì³ ìŒìœ¼ë¡œ ì´ë£¨ì–´ì§„ ìƒí˜¸ì‘ìš©ì„ í¬ì°©í•˜ë„ë¡ ì„¤ê³„ëœ ì´ìƒ ìŠ¤ì½”ì–´ë§ í•¨ìˆ˜
            
            ![Untitled](Deep%20Learn%2020e25/Untitled%2034.png)
            
            - z = xì˜ ië²ˆì§¸ í•ì³ ê°’ì˜ ì €ì°¨ì› ì„ë² ë”©
            - $w_{ij}$ = ìƒí˜¸ì‘ìš©ì— ê°€ì¤‘ì¹˜ë¥¼ ë”í•œ ê²ƒìœ¼ë¡œ íŠ¸ë ˆì´ë‹ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°
    - ì£¼ë¡œ ë²”ì£¼í˜• ë°ì´í„°ì—ì„œ ì´ìƒì„ íƒì§€í•˜ë„ë¡ ì„¤ê³„ ë¨
    - Advantages.
        1. ìƒí˜¸ì‘ìš©ì˜ ë‹¤ì–‘í•œ ì¢…ë¥˜ê°€ ì´ìƒ ì ìˆ˜ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ë¡œ í†µí•©ë  ìˆ˜ ìˆë‹¤.
        2. ì´ìƒ ì ìˆ˜ëŠ”  ìš°ë¦¬ê°€ í¬ì°©í•˜ë ¤ëŠ” íŠ¹ì • ì´ìƒ ìƒí˜¸ì‘ìš© ê´€í•˜ì—¬ ì¶©ì‹¤í•˜ê²Œ ìµœì í™” ëœë‹¤.
    - Disadvantages.
        1. ìƒí˜¸ì‘ìš©ì˜ ê³„ì‚°ì€ ê° ë°ì´í„°ì˜ ì¸ìŠ¤í„´ìŠ¤ì˜ í•ì³/ìš”ì†Œê°€ ë§ì€ ê²½ìš° ë§ì´ ì†Œìš” ë  ìˆ˜ ìˆìŒ
        2. ì´ìƒ ì ìˆ˜ í•™ìŠµì€ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œ ìƒì„± í’ˆì§ˆì— í¬ê²Œ ì¢Œìš°ëœë‹¤.
    - Challenges Targeted
        1. CH2 and CH5(ê³ ì°¨ì›/ë¹„ë…ë¦½ and ë³µì¡) : ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ê°€ì§„ ë°ì´í„°ì…‹ì˜ ì €ì°¨ì› í‘œí˜„ì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ìœ ë§í•œ ë°©ë²• ì œê³µ
        2. CH1(ë‚®ì€ ê°ì§€ìœ¨) : í•™ìŠµëœ í‘œí˜„ì€ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ë” ë§ì€ ì •ìƒ/ë¹„ì •ìƒì„± ì •ë³´ë¥¼ í¬ì°©í•¨ìœ¼ë¡œ ë” ë‚˜ì€ ê°ì§€ë¥¼ í•  ìˆ˜ ìˆê²Œ í•œë‹¤.
    
4. End-to-end One-class Classification
    - end-to-end ë°©ì‹ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ê°€ ì •ìƒì¸ì§€ ì•„ë‹Œì§€ë¥¼ íŒë³„í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ëŠ” one-class classifierë¥¼ í›ˆë ¨í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•¨
    - ex) GAN + one-class classification ì¦‰, adversarially learned one-class classification
    - GAN-based methodsëŠ” ìš°ì„  ì‹¤ì œ ë°ì´í„° ë¶„í¬ì™€ ê·¼ì‚¬í•˜ê²Œ ìƒì„± ë¶„í¬ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ë©°, ì •ê·œ ì¸ìŠ¤í„´ìŠ¤ì™€ ì ëŒ€ì ìœ¼ë¡œ ìƒì„±ëœ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë¶„ë¦¬í•˜ê¸° ìœ„í•´ ì°¨ë³„ì  ëª¨ë¸ì„ ìµœì í™”í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•¨
    - ì¢…ë¥˜ : adversarially learned one-class classification(ALOCC), One-class adversarial networks (OCAN)
    - Assumptions.
        1. ì´ìƒì¹˜ì— ê·¼ì‚¬ëœ ë°ì´í„° ì¸ìŠ¤í„´ìŠ¤ëŠ” íš¨ê³¼ì ìœ¼ë¡œ í•©ì„±í•  ìˆ˜ ìˆë‹¤.
        2. ëª¨ë“  ì •ê·œ ì¸ìŠ¤í„´ìŠ¤ë¥¼ discriminative one-class modelë¡œ ìš”ì•½í•  ìˆ˜ ìˆë‹¤.
    - The one-class modelì€ íŒë³„ì ë„¤íŠ¸ì›Œí¬ì— êµ¬ì¶•ë˜ê³ , ìƒì„±ì ë„¤íŠ¸ì›Œí¬ëŠ” ë…¸ì´ì¦ˆ ì œê±° AEì— ê¸°ë°˜í•œë‹¤.
    - The objective of the AE-empower GAN
        
        ![Untitled](Deep%20Learn%2020e25/Untitled%2035.png)
        
        - $p_{\hat{X}}$ = ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆì— ì˜í•´ ì†ìƒëœ  Xì˜ ë°ì´í„° ë¶„í¬
        - ìœ„ì˜ í•¨ìˆ˜ëŠ” ì•„ë˜ì˜ data construction error in AEì™€ ê°™ì´ ìµœì í™” ë¨
            
            ![Untitled](Deep%20Learn%2020e25/Untitled%2036.png)
            
    - Advantages.
        1. ì´ìƒ ë¶„ë¥˜ ëª¨ë¸ì€ end-to-end ë°©ì‹ìœ¼ë¡œ ì ëŒ€ì ìœ¼ë¡œ ìµœì í™”ëœë‹¤.
        2. ì ëŒ€ì  í•™ìŠµê³¼ one-class ë¶„ë¥˜ê¸°ì˜ í’ë¶€í•œ ê¸°ìˆ ê³¼ ì´ë¡ ìœ¼ë¡œ ë°œì „, ì§€ì›ë  ìˆ˜ ìˆë‹¤.
    - Disadvantages.
        1. ì•Œë ¤ì§€ì§€ ì•Šì€ ì´ìƒì¹˜ì™€ ìœ ì‚¬í•˜ê²Œ ìƒì„±ì„ ë³´ì¦ í•˜ëŠ”ê±´ ì–´ë ¤ìš¸ ìˆ˜ ìˆë‹¤.
        2. GANì˜ ë¶ˆì•ˆì •ì„±ìœ¼ë¡œ ì¸í•´ ëŒœì–‘í•œ í’ˆì§ˆì„ ê°€ì§„ ì¸ìŠ¤í„´ìŠ¤ê°€ ë°œìƒí•˜ì—¬ ì´ìƒ ë¶„ë¥˜ ì„±ëŠ¥ì´ ë¶ˆì•ˆì •í•´ì§ˆ ìˆ˜ ìˆë‹¤.
        3. ì¤€ì§€ë„ ì´ìƒì¹˜ íƒì§€ ì‹œë‚˜ë¦¬ì˜¤ ì‘ìš©ì— ì œí•œëœë‹¤.
    - Challenges Targeted
        1. CH1 and CH2(ë‚®ì€ ê°ì§€ìœ¨ê³¼ ê³ ì°¨ì› ë¹„ë…ë¦½) : ì ëŒ€ì  í•™ìŠµëœ one-class classifiersì€ í˜„ì‹¤ì ì¸ ê²½ê³„ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ê³ , ì €ì°¨ì› ì •ê·œì„± í‘œí˜„ì„ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.
        

# ALGORITHMS AND DATASETS

1. Representative Algorithms (30ê°€ì§€ ëŒ€í‘œì ì¸ ì•Œê³ ë¦¬ì¦˜ì˜ ì£¼ìš” íŠ¹ì§•)
    
    ![Untitled](Deep%20Learn%2020e25/Untitled%2037.png)
    
    1.  ëŒ€ë¶€ë¶„ì˜ ë°©ë²•ì€ ë¹„ì§€ë„ ë˜ëŠ” ì¤€ì§€ë„ ëª¨ë“œë¡œ ìš´ì˜ë¨
    2. ë°ì´í„° ì¦ê°•, ë“œë¡­ ì•„ì›ƒ ë° ì‚¬ì „ í›ˆë ¨ê³¼ ê°™ì€ ë”¥ëŸ¬ë‹ íŠ¸ë¦­ì´ ì¶©ë¶„íˆ ì—°êµ¬ë˜ì§€ ì•Šì•˜ìŒ
    3. ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜ê°€ ê¹Šì§€ ì•Šë‹¤. ëŒ€ë¶€ë¶„ ë°©ë²•ì´ 5ê°œ ì´í•˜ì˜ ë„¤íŠ¸ì›Œí¬ ë ˆì´ì–´ë¥¼ ê°€ì§
    4. ReLuëŠ” ê°€ì¥ ì¸ê¸° ìˆëŠ” í™œì„±í™” í•¨ìˆ˜ì´ë‹¤
    5. ë‹¤ì–‘í•œ íƒ€ì…ì˜ ì¸í’‹ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ backbone ë„¤íŠ¸ì›Œí¬ ì‚¬ìš© ê°€ëŠ¥.
    
2. Datasets with Real Anomalies : ì‹¤ì œ ì´ìƒì¹˜ì— ëŒ€í•œ ê³µê°œ ê°€ëŠ¥ 21ê°œì˜ ì‹¤ì œ ë°ì´í„° ì„¸íŠ¸
    
    ![Untitled](Deep%20Learn%2020e25/Untitled%2038.png)
    

# CONCLUSIONS AND FUTURE OPPORTUNITIES

- ì´ìƒì¹˜ ê°ì§€ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹ ê¸°ìˆ  í™œìš©ì— ëŒ€í•œ 12ê°€ì§€ì˜ ë‹¤ì–‘í•œ ëª¨ë¸ë§ ê´€ì ì„ ê²€í†  í–ˆë‹¤.

1. Exploring Anomaly-supervisory Signals : ì´ìƒ ê°ì§€ ì‹ í˜¸ ì¡°ì‚¬
    - 5.1 : objective functionsì´ ì¼ë°˜ì ì´ì§€ë§Œ, ì´ìƒì¹˜ ê°ì§€ì— íŠ¹íˆ ìµœì í™” ë˜ì§€ ì•ŠëŠ” ë¬¸ì œ
    - 5.2 : ì œì•½ì„ ê°€í•¨ìœ¼ë¡œ ìœ„ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ”ë° ë„ì›€ì„ ì£¼ë ¤ í–ˆì§€ë§Œ, í•œê³„ê°€ ìˆìŒ
    - data reconstruction and GANsì˜ í˜•ì‹ì„ ë²—ì–´ë‚˜ ì´ìƒ ë¶„í¬ì˜ ì•½í•œ ê°€ì •ì„ ê°€ì§„ ìƒˆë¡œìš´ ì†ŒìŠ¤ë¥¼ íƒìƒ‰í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.
    
2. Deep Weakly Supervised Anomaly Detection
    - Deep weakly supervised anomaly detection
        
        : ì¼ë¶€/ë¶€ì •í™•/ë¶ˆí™•ì‹¤í•˜ê²Œ ë¼ë²¨ì´ ë¶€ì°©ëœ ì´ìƒ ë°ì´í„° ì¦‰, weakly supervised anomaly signalsë¥¼ deep neural networksë¥¼ í™œìš©í•˜ì—¬ ì´ìƒ-ì •ë„ ê°ì§€ ëª¨ë¸ì„ í•™ìŠµí•˜ë ¤ê³  í•¨.
        
    - unknown anomaly detectioní•˜ëŠ” ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•¨
    - data-efficient anomaly detection & few-shot anomaly detectionì„ ë°œì „ì‹œí‚¤ëŠ” ê²ƒì´ ì¤‘ìš”í•¨
    
3. Large-scale Normality Learning
    - Large-scale ë¹„ì§€ë„ representation learningì€ downstream learning tasksì—ì„œ ê±°ëŒ€í•œ ì„±ê³µì„ ê±°ë‘ì—ˆë‹¤.
    - 2.1 : ì¶©ë¶„í•œ ë¼ë²¨ëœ ë°ì´í„°ë¥¼ ì–»ê¸° í˜ë“  ì´ìƒì¹˜ íƒì§€ì¸ í•™ìŠµ ê³¼ì œì—ì„œ íŠ¹íˆ ì¤‘ìš”í•˜ë‹¤.
    - ëª©í‘œ : ë¹„ì§€ë„ í•™ìŠµì—ì„œ ë ˆì´ë¸”ì´ ì—†ëŠ” ëŒ€ê·œëª¨ ë°ì´í„°ë¥¼ í†µí•´ í‘œí˜„ í•™ìŠµ ëª¨ë¸ì„ ì‚¬ì „ í•™ìŠµí•˜ê³ , ì¤€ì§€ë„ í•™ìŠµì„ í†µí•´ ê°ì§€ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒì´ë‹¤. (ë‹¨ ë°ì´í„°ì— ì˜¤ì—¼ì´ ì—†ì–´ì•¼ í•¨)
    
4. Deep Detection of Complex Anomalies
    - ëŒ€ë¶€ë¶„ì˜ ì´ìƒì¹˜ ê°ì§€ ë°©ë²•ë“¤ì€ Point anomaliesê°€ ì—°êµ¬ë˜ì–´ ì™”ê³ , conditional/group anomaliesì€ ëœ ì—°êµ¬ë˜ì–´ì¡Œë‹¤.
    - ë”¥ëŸ¬ë‹ì€ ë³µì¡í•œ ì‹œê°„ì /ê³µê°„ì  ì˜ì¡´ê³¼ unordered data points ì„¸íŠ¸ì˜ í‘œí˜„ì„ í¬ì°©í•˜ê³  í•™ìŠµí•˜ëŠ” ë›°ì–´ë‚œ ëŠ¥ë ¥ì„ ê°€ì¡Œë‹¤. â†’ ìƒˆë¡œìš´ NN ë˜ëŠ” objectives functionsê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ
    - Multimodal anomaly detectionì€ ë¯¸ê°œì²™ ì—°êµ¬ ì˜ì—­ì´ë‹¤.
    - ë‹¤ë¥¸ ë°ì´í„° ì†ŒìŠ¤ í‘œí˜„ì„ ì—°ê²°í•˜ì—¬ í†µí•© í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ë“± deep ì ‘ê·¼ì€ multimodal anomaly detectionì— ì¤‘ìš”í•œ ê¸°íšŒë¥¼ ì œê³µí•œë‹¤.
    
5. Interpretable and Actionable Deep Anomaly Detection
    - í•´ì„ ê°€ëŠ¥ & ì‹¤í–‰ ê°€ëŠ¥ DADëŠ” ê²°ê³¼ì™€ ëª¨ë¸ ê²°ì • ì´í•´ë¥¼ ìœ„í•´ í•„ìˆ˜ì ì´ë‹¤.
    - ì´ìƒì„ ê°€ì¥ ë¹„ìƒì ìœ¼ë¡œ ë§Œë“œëŠ” í•ì³ì˜ ì„œë¸Œì…‹ì„ ì°¾ì•„ì„œ ì´ìƒì¹˜ ì„¤ëª… ë¬¸ì œë¥¼ í‘¸ëŠ” ì—°êµ¬ê°€ ìˆì—ˆë‹¤.
    - í•˜ì§€ë§Œ í•´ì„ì„±ê³¼ ì¡°ì¹˜ì„±ì´ ì•½í•˜ê¸° ë•Œë¬¸ì— ì´ìƒ ì„¤ëª…ì„ ì œê³µí•˜ëŠ” ê³ ìœ  ê¸°ëŠ¥ì„ ê°€ì§„ ì‹¬ì¸µ ëª¨ë¸ì´ í•„ìš”í•¨
    
6. Novel Applications and Settings (ìƒˆë¡œìš´ ì‘ìš© ë° ì„¤ì •)
    1. out-of-distribution (OOD) detection
        
        : í›ˆë ¨ ë¶„í¬ë¡œë¶€í„° ë¨¼ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°ì§€í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ML systemì´ ì‹¤ì œ í™˜ê²½ì— ìƒˆë¡œìš´ í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” í•„ìˆ˜ ê¸°ìˆ ì´ë‹¤. OODë¥¼ í†µí•´ ì´ìƒ ê°ì§€ ë¿ë§Œ ì•„ë‹Œ ì„¸ë¶„í™”ëœ ì •ìƒ ë“±ê¸‰ í´ë˜ìŠ¤ ë¼ë²¨ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ê³  ê°€ì •í•œë‹¤.
        
    2. curiosity learning
        
        : ë³´ë„ˆìŠ¤ ë³´ìƒ í•¨ìˆ˜ë¥¼ ê°•í™” í•™ìŠµì—ì„œ í¬ë°•í•œ ë³´ìƒê³¼ í•¨ê»˜ ë°°ìš°ëŠ” ê²ƒ
        
        - ê°•í™”í•™ìŠµì€ ë³´ìƒì´ í¬ë°•í•œ í™˜ê²½ì—ì„œ ì˜ ì‘ë™í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ë³´ë„ˆìŠ¤ ë³´ìƒì„ í†µí•´ í™˜ê²½ì„ ê°•í™”í•¨ìœ¼ë¡œì¨ ê°•í™”í•™ìŠµ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤.
        - ë³´ë„ˆìŠ¤ ë³´ìƒì€ ì‹ ê·œì„± ë˜ëŠ” í¬ê·€ì„±ì— ê¸°ì´ˆí•˜ì—¬ ì •ì˜ ë¨. ì¦‰ novel/rare statesë¥¼ ë°œê²¬í•˜ë©´ í°  ë³´ë„ˆìŠ¤ ë³´ìƒì„ ë°›ìŒ
    3. non- independent and identically distributed (IID) : ë…ë¦½ì /ë™ì¼í•œ ë¶„í¬ê°€ ì•„ë‹Œ
        - ì‹¤ì œ ì¸ìŠ¤í„´ìŠ¤ë“¤ì€ non-IID í•˜ë‹¤. ì´ëŸ¬í•œ ì¸ìŠ¤í„´ìŠ¤ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ non-IID anomaly detectionì´ í•„ìš”í•¨.
        - ë³µì¡í•œ ìƒí™©ì—ì„œ non-IID íŠ¹ì„±ì„ ê³ ë ¤í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤.
    4. other interesting applications 
        - detection of adversarial examples
        - anti-spoofing in biometric systems : ìƒì²´ ì¸ì‹ ì‹œìŠ¤í…œì˜ ìŠ¤í‘¸í•‘(ê±°ì§“ IP) ë°©ì§€
        - early detection of rare catastrophic events : í¬ê·€ ì¬ì•™ì  ì‚¬ê±´ì— ëŒ€í•œ ì¡°ê¸° ê°ì§€
            - e.g., financial crisis(ê¸ˆìœµ ìœ„ê¸°) and other black swan events(í‘ì¡° ì´ë¡ )
            

![Untitled](Deep%20Learn%2020e25/Untitled%2039.png)

- ë¹„ì •í˜• ë°ì´í„° :  Vision, NLP
- ë”¥ëŸ¬ë‹ì€ ì¸í’‹ í˜•íƒœê°€ ìƒê´€ì—†ë‹¤
- í•˜ì´ë¸Œë¦¬ë“œ : ë”¥ëŸ¬ë‹ í”¼ì…” ìµìŠ¤íŠ¸ë™í„° + ë¨¸ì‹ ëŸ¬ë‹ : END - END ë¶ˆê°€ëŠ¥

![Untitled](Deep%20Learn%2020e25/Untitled%2040.png)
